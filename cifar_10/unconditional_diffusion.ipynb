{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset \n",
    "# We apply the lambda rescaling below so that each pixel from the [0, 1] range rescales to the range [1, -1]. \n",
    "# During the forward process, DDPMs add Gaussian noise ~ N (0, 1) to the images. So having input images \n",
    "# in [-1, 1] makes the pixel space more centered and symmetric, aligning better with the noise distribution. \n",
    "# This allows for improved training stability. \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Lambda(lambda x: x * 2. - 1.)\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root = './data', train = True, download = True, transform = transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size = 128, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard U-Net with downsampling and upsampling \n",
    "class UNet(nn.Module): \n",
    "    def __init__(self, in_channels = 3, base_channels = 64): \n",
    "        super().__init__()\n",
    "        # Downsampling path \n",
    "        # We add 1 to the in_channels for self.enc1 because we need to account for the time dimension\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels + 1, base_channels, 3, padding = 1), nn.ReLU(), \n",
    "            nn.Conv2d(base_channels, base_channels, 3, padding = 1), nn.ReLU()\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels, base_channels * 2, 3, padding = 1), nn.ReLU(),\n",
    "            nn.Conv2d(base_channels * 2, base_channels * 2, 3, padding = 1), nn.ReLU()\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck \n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv2d(base_channels * 2, base_channels * 4, 3, padding = 1), nn.ReLU(), \n",
    "            nn.Conv2d(base_channels * 4, base_channels * 4, 3, padding = 1), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Upsampling path \n",
    "        self.up1 = nn.ConvTranspose2d(base_channels * 4, base_channels * 2, 2, stride = 2)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels * 4, base_channels * 2, 3, padding = 1), nn.ReLU(), \n",
    "            nn.Conv2d(base_channels * 2, base_channels * 2, 3, padding = 1), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(base_channels * 2, base_channels, 2, stride = 2)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels * 2, base_channels, 3, padding = 1), nn.ReLU(), \n",
    "            nn.Conv2d(base_channels, base_channels, 3, padding = 1), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.final = nn.Conv2d(base_channels, in_channels, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, t): \n",
    "        t = t[:, None, None, None].repeat(1, 1, x.shape[2], x.shape[3])\n",
    "        x = torch.cat([x, t], dim =1)\n",
    "\n",
    "        e1 = self.enc1(x)   # -> [B, base, 32, 32]\n",
    "        e2 = self.enc2(self.pool1(e1))  # -> [B, base * 2, 16, 16]\n",
    "        mid = self.middle(self.pool2(e2))   # -> [B, base*4, 8, 8]\n",
    "\n",
    "        d1 = self.up1(mid)  # -> [B, base*2, 16, 16]\n",
    "        d1 = self.dec1(torch.cat([d1, e2], dim = 1))\n",
    "\n",
    "        d2 = self.up2(d1)   # -> [B, base, 32, 32]\n",
    "        d2 = self.dec2(torch.cat([d2, e1], dim = 1))\n",
    "\n",
    "        return self.final(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffusion schedule \n",
    "T = 1000 \n",
    "\n",
    "# Controls how much Gaussian noise is added at each time step \n",
    "# Smaller betas = gentler corruption per step \n",
    "betas = torch.linspace(1e-4, 0.02, T) \n",
    "\n",
    "# Represent the amount of signal retained at each time step \n",
    "alphas = 1. - betas\n",
    "\n",
    "# This is a cumulative product over all previous alphas \n",
    "# It tells you how much of the original image is preserved at time step t in expectation \n",
    "alphas_cumprod = torch.cumprod(alphas, dim = 0)\n",
    "\n",
    "# In the forward pass of diffusion, x_t = sqrt(cum_prod(alpha)) * x_0 + sqrt(1 - cumprod(alpha)) * epsilon \n",
    "# x_0 is the original image; epsilon is the Gaussian noise \n",
    "def q_sample(x0, t, noise = None): \n",
    "    if noise is None: \n",
    "        noise = torch.randn_like(x0)\n",
    "    sqrt_alpha = torch.sqrt(alphas_cumprod[t])[:, None, None, None]\n",
    "    sqrt_one_minus = torch.sqrt(1 - alphas_cumprod[t])[:, None, None, None]\n",
    "    return sqrt_alpha * x0 + sqrt_one_minus * noise \n",
    "# Early in the diffusion stage (small t): x_t is close to x_0\n",
    "# Late in the diffusion stage (large t): x_t is almost pure noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = [] \n",
    "for epoch in range(num_epochs): \n",
    "    epoch_losses = [] \n",
    "    for x, _ in tqdm(train_loader, desc = f\"Epoch{epoch +1}\"):\n",
    "        x = x.to(device)\n",
    "\n",
    "        # t is a batch of random timesteps. In DDPM training, we want our model \n",
    "        # to learn to denoise for every possible timestep t from 0 to T-1. \n",
    "        # So we randomly pick a timestep for each image in the batch. \n",
    "        # We corrupt the image to get x_t using q_sample(x_0, t)\n",
    "        # We then train the model to predict the noise added at that specific t. \n",
    "        # This is how the model learns the entire denoising trajectory, one random step at a time. \n",
    "        t = torch.randint(0, T, (x.size(0),), device = device).long()\n",
    "        noise = torch.randn_like(x)\n",
    "        x_t = q_sample(x, t, noise)\n",
    "        t_input = t.float()/T\n",
    "        pred = model(x_t, t_input)\n",
    "        loss = F.mse_loss(pred, noise)\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        \n",
    "    loss_history.append(np.mean(epoch_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow-diffusion-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
