{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device and hyperparameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128\n",
    "num_epochs = 50\n",
    "learning_rate = 0.0002\n",
    "beta_start = 0.0001\n",
    "beta_end = 0.02\n",
    "timesteps = 1000\n",
    "base_channels = 64\n",
    "ddim_steps = [10, 50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, transform=transform, download=True\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Unconditional U-Net for CIFAR-10\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, base_channels=64):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(3, base_channels, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(base_channels, base_channels, 3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels, base_channels * 2, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(base_channels * 2, base_channels * 2, 3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv2d(base_channels * 2, base_channels * 4, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(base_channels * 4, base_channels * 4, 3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(base_channels * 4, base_channels * 2, 2, stride=2)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels * 4, base_channels * 2, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(base_channels * 2, base_channels * 2, 3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(base_channels * 2, base_channels, 2, stride=2)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(base_channels * 2, base_channels, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(base_channels, base_channels, 3, padding=1), nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.final = nn.Conv2d(base_channels, 3, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        p1 = self.pool1(e1)\n",
    "        e2 = self.enc2(p1)\n",
    "        p2 = self.pool2(e2)\n",
    "        \n",
    "        m = self.middle(p2)\n",
    "        \n",
    "        u1 = self.up1(m)\n",
    "        d1_input = torch.cat([u1, e2], dim=1)\n",
    "        d1 = self.dec1(d1_input)\n",
    "        \n",
    "        u2 = self.up2(d1)\n",
    "        d2_input = torch.cat([u2, e1], dim=1)\n",
    "        d2 = self.dec2(d2_input)\n",
    "        \n",
    "        return torch.tanh(self.final(d2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffusion schedule\n",
    "def get_betas(timesteps):\n",
    "    betas = torch.linspace(beta_start, beta_end, timesteps)\n",
    "    return betas.to(device)\n",
    "\n",
    "def get_alphas(betas):\n",
    "    alphas = 1.0 - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "    return alphas, alphas_cumprod\n",
    "\n",
    "def q_sample(x_start, t, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_start)\n",
    "    \n",
    "    betas = get_betas(timesteps)\n",
    "    alphas, alphas_cumprod = get_alphas(betas)\n",
    "    \n",
    "    sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod[t]).view(-1, 1, 1, 1)\n",
    "    sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod[t]).view(-1, 1, 1, 1)\n",
    "    \n",
    "    sqrt_alphas_cumprod = sqrt_alphas_cumprod.expand_as(x_start)\n",
    "    sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod.expand_as(x_start)\n",
    "    \n",
    "    return (sqrt_alphas_cumprod * x_start + sqrt_one_minus_alphas_cumprod * noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "model = UNet(base_channels).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "betas = get_betas(timesteps)\n",
    "alphas, alphas_cumprod = get_alphas(betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with loss tracking\n",
    "def train_model():\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for i, (images, _) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            batch_size = images.shape[0]\n",
    "            \n",
    "            t = torch.randint(0, timesteps, (batch_size,), device=device)\n",
    "            \n",
    "            noise = torch.randn_like(images)\n",
    "            x_noisy = q_sample(images, t, noise)\n",
    "            predicted_noise = model(x_noisy)\n",
    "            \n",
    "            loss = criterion(predicted_noise, noise)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDIM sampling with trajectory\n",
    "@torch.no_grad()\n",
    "def sample_ddim(model, num_samples=1, ddim_steps=100, eta=0.0, save_trajectory=False):\n",
    "    model.eval()\n",
    "    x = torch.randn(num_samples, 3, 32, 32).to(device)\n",
    "    \n",
    "    step_size = timesteps // ddim_steps\n",
    "    timesteps_subset = list(range(timesteps - 1, -1, -step_size))\n",
    "    if timesteps_subset[-1] != 0:\n",
    "        timesteps_subset[-1] = 0\n",
    "    \n",
    "    trajectory = [x.cpu().numpy()] if save_trajectory else None\n",
    "    \n",
    "    for i in range(len(timesteps_subset) - 1):\n",
    "        t = timesteps_subset[i]\n",
    "        t_next = timesteps_subset[i + 1]\n",
    "        \n",
    "        t_tensor = torch.full((num_samples,), t, device=device)\n",
    "        \n",
    "        predicted_noise = model(x)\n",
    "        \n",
    "        alpha_cumprod_t = alphas_cumprod[t]\n",
    "        alpha_cumprod_t_next = alphas_cumprod[t_next]\n",
    "        \n",
    "        sigma_t = eta * torch.sqrt((1 - alpha_cumprod_t_next) / (1 - alpha_cumprod_t) * (1 - alpha_cumprod_t / alpha_cumprod_t_next))\n",
    "        \n",
    "        x0_t = (x - torch.sqrt(1 - alpha_cumprod_t) * predicted_noise) / torch.sqrt(alpha_cumprod_t)\n",
    "        direction_pointing_to_xt = torch.sqrt(1 - alpha_cumprod_t_next - sigma_t**2) * predicted_noise\n",
    "        \n",
    "        x = torch.sqrt(alpha_cumprod_t_next) * x0_t + direction_pointing_to_xt\n",
    "        \n",
    "        if sigma_t > 0:\n",
    "            x = x + sigma_t * torch.randn_like(x)\n",
    "        \n",
    "        if save_trajectory and (t % 100 == 0 or t == 0):\n",
    "            trajectory.append(x.cpu().numpy())\n",
    "    \n",
    "    return x, trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples with different DDIM steps\n",
    "fig, axes = plt.subplots(1, len(ddim_steps), figsize=(15, 5))\n",
    "for i, steps in enumerate(ddim_steps):\n",
    "    samples, _ = sample_ddim(model, num_samples=1, ddim_steps=steps)\n",
    "    samples = samples.cpu().numpy()\n",
    "    ax = axes[i]\n",
    "    ax.imshow(np.transpose(samples[0], (1, 2, 0)) * 0.5 + 0.5)\n",
    "    ax.set_title(f'DDIM Steps: {steps}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Samples with Different DDIM Steps\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trajectory with DDIM (using 100 steps)\n",
    "samples, trajectory = sample_ddim(model, num_samples=1, ddim_steps=100, save_trajectory=True)\n",
    "trajectory = np.array(trajectory)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(trajectory), figsize=(15, 3))\n",
    "for i in range(len(trajectory)):\n",
    "    img = trajectory[i][0]\n",
    "    img = np.transpose(img, (1, 2, 0)) * 0.5 + 0.5\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f'Step {timesteps - (i * 100)}')\n",
    "    axes[i].axis('off')\n",
    "plt.suptitle(\"Trajectory with DDIM (100 Steps)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow-diffusion-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
